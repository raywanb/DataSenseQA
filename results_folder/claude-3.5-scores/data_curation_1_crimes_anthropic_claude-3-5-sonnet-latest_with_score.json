[
    {
        "question": "How many columns does the dataset contain and how many columns contain null values? Print the result as 'Number of Columns' and 'Number of Columns with Null Value'.",
        "ground_truth": "Number of Columns: 22, Number of Columns with Null Value: 7 ",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Number of Columns: 22\nNumber of Columns with Null Value: 7",
        "score": 1
    },
    {
        "question": "Which column has the most missing values? Print the column name.",
        "ground_truth": "X Coordinate",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Location (with 2,352 missing values)",
        "score": 0
    },
    {
        "question": "Calculate the ratio of missing to non-missing values for each column. Print the result as 'Column Name: Ratio'.",
        "ground_truth": "ID: 0.0\nCase Number: 0.0\nDate: 0.0\nBlock: 0.0\nIUCR: 0.0\nPrimary Type: 0.0\nDescription: 0.0\nLocation Description: 0.0051\nArrest: 0.0\nDomestic: 0.0\nBeat: 0.0\nDistrict: 0.0\nWard: 0.0001\nCommunity Area: 0.0\nFBI Code: 0.0\nX Coordinate: 0.0091\nY Coordinate: 0.0091\nYear: 0.0\nUpdated On: 0.0\nLatitude: 0.0091\nLongitude: 0.0091\nLocation: 0.0091",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Here are the ratios of missing to non-missing values for each column:\n\nID: 0.0000 (no missing values)\nCase Number: 0.0000 (no missing values)\nDate: 0.0000 (no missing values)\nBlock: 0.0000 (no missing values)\nIUCR: 0.0000 (no missing values)\nPrimary Type: 0.0000 (no missing values)\nDescription: 0.0000 (no missing values)\nLocation Description: 0.0051 (0.51% ratio of missing to non-missing)\nArrest: 0.0000 (no missing values)\nDomestic: 0.0000 (no missing values)\nBeat: 0.0000 (no missing values)\nDistrict: 0.0000 (no missing values)\nWard: 0.0001 (0.01% ratio of missing to non-missing)\nCommunity Area: 0.0000 (no missing values)\nFBI Code: 0.0000 (no missing values)\nX Coordinate: 0.0091 (0.91% ratio of missing to non-missing)\nY Coordinate: 0.0091 (0.91% ratio of missing to non-missing)\nYear: 0.0000 (no missing values)\nUpdated On: 0.0000 (no missing values)\nLatitude: 0.0091 (0.91% ratio of missing to non-missing)\nLongitude: 0.0091 (0.91% ratio of missing to non-missing)\nLocation: 0.0091 (0.91% ratio of missing to non-missing)\n\nThe location-related columns (X Coordinate, Y Coordinate, Latitude, Longitude, Location) have the highest ratio of missing to non-missing values at 0.91%, followed by Location Description at 0.51%. Most columns have no missing values at all.",
        "score": 1
    },
    {
        "question": "Name all columns that contain null values and calculate the percentage to non null values rounded to three values after the comma. Print the result as 'Column Name' : 'Percentage'.",
        "ground_truth": "'Location Description': '0.511%'\n'Ward': '0.006%'\n'X Coordinate': '0.908%'\n'Y Coordinate': '0.908%'\n'Latitude': '0.908%'\n'Longitude': '0.908%'\n'Location': '0.908%'",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Here are all columns containing null values and their percentage of null values:\n'Location Description' : 0.509%\n'Ward' : 0.006%\n'X Coordinate' : 0.899%\n'Y Coordinate' : 0.899%\n'Latitude' : 0.899%\n'Longitude' : 0.899%\n'Location' : 0.899%\n\nWe can observe that:\n1. 'Location Description' has about 0.509% null values\n2. 'Ward' has the lowest percentage of null values at 0.006%\n3. All location-related columns (X Coordinate, Y Coordinate, Latitude, Longitude, and Location) have the same percentage of null values at 0.899%, which suggests these values are likely missing together when geographic data couldn't be recorded.",
        "score": 0
    },
    {
        "question": "Find and print the count of all rows where 'Latitude' and 'Longitude' are both missing (NaN).",
        "ground_truth": "2352",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "There are 2,352 rows in the dataset where both Latitude and Longitude coordinates are missing (NaN).",
        "score": 1
    },
    {
        "question": "Check if there are any duplicate rows and count them.",
        "ground_truth": "0",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "There are 0 duplicate rows in the dataframe. This means each row in the dataset is unique.",
        "score": 1
    },
    {
        "question": "Check if there are any duplicate 'Case Number' values in the dataset. Print YES or NO.",
        "ground_truth": "YES",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "YES",
        "score": 1
    },
    {
        "question": "Print all IDs of the rows of the first ten rows where the crime was not Theft.",
        "ground_truth": "[11662417, 12990873, 11630496, 11632505, 11765926, 11885224, 13210699, 13211437, 13211898, 11842146]",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "The IDs of non-theft crimes in the first 10 rows are: 11662417, 12990873, 11885224, 13210699, 13211437, 13211898, 11842146",
        "score": 0
    },
    {
        "question": "Find all the columns that only contain the same value in each row. Print the result as 'Columns with the same value in all rows:' : 'Column Name' or None if no such column exists.",
        "ground_truth": "Columns with the same value in all rows: 'Year'",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Columns with the same value in all rows: 'Year'",
        "score": 1
    },
    {
        "question": "How many unique values exist in the 'Location Description' column? Print the count.",
        "ground_truth": "156",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "There are 156 unique values in the 'Location Description' column.",
        "score": 1
    },
    {
        "question": "Identify and count records where the 'Case Number' format does not match the pattern 'XX999999'. Print the count.",
        "ground_truth": "1",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "There is 1 record in the dataset where the Case Number does not match the pattern 'XX999999' (two letters followed by six digits). The non-matching case number is 'C475067', which starts with only one letter instead of two letters followed by six digits.",
        "score": 1
    },
    {
        "question": "Standardize 'Location' column values to the format '(latitude, longitude)'. Print the first five formatted values in an array.",
        "ground_truth": "['(41.749500329, -87.6011574)', '(41.89621515, -87.728572048)', None, None, '(41.736126864, -87.629184056)']",
        "derivation": "",
        "type": "data curation",
        "subtype": "data normalization and standardization",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "The first five formatted location values are:\n1. (41.749500329, -87.6011574)\n2. (41.89621515, -87.728572048)\n3. None\n4. None\n5. (41.736126864, -87.629184056)\n\nThe format has been standardized to show the latitude and longitude in parentheses, with the values kept at their original precision. Rows where the location was originally NaN remain as None values.",
        "score": 1
    },
    {
        "question": "Ensure that timestamps in the 'Date' column are rounded to the nearest minute. Print the first five updated timestamps.",
        "ground_truth": "['2019-04-21 12:30:00', '2019-08-17 13:14:00','2019-03-16 11:35:00', '2019-03-20 01:00:00', '2019-07-21 14:00:00']",
        "derivation": "",
        "type": "data curation",
        "subtype": "data normalization and standardization",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "The timestamps in the 'Date' column have been successfully converted to datetime format and rounded to the nearest minute. Here are the first five timestamps:\n1. 2019-04-21 12:30:00\n2. 2019-08-17 13:14:00\n3. 2019-03-16 11:35:00\n4. 2019-03-20 01:00:00\n5. 2019-07-21 14:00:00\n\nThe timestamps are now in a consistent datetime format (YYYY-MM-DD HH:MM:SS) and have been rounded to the nearest minute. Note that in this case, the original times were already recorded to the minute, so no actual rounding was necessary.",
        "score": 1
    },
    {
        "question": "Partition the dataset by 'District' and count the number of crimes in each partition. Print 'District: Count'.",
        "ground_truth": "'11': 18735\n'6': 16900\n'8': 15743\n'1': 15309\n'18': 15181\n'4': 14074\n'7': 13746\n'12': 13197\n'25': 12968\n'3': 12526\n'10': 12508\n'19': 11849\n'5': 11388\n'2': 11267\n'9': 11031\n'15': 10022\n'14': 9296\n'16': 8326\n'22': 8267\n'24': 8032\n'17': 6764\n'20': 4389\n'31': 7",
        "derivation": "",
        "type": "data curation",
        "subtype": "data filtering and partitioning",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Here are the crime counts for each district:\nDistrict 1: 15,309 crimes\nDistrict 2: 11,267 crimes\nDistrict 3: 12,526 crimes\nDistrict 4: 14,074 crimes\nDistrict 5: 11,388 crimes\nDistrict 6: 16,900 crimes\nDistrict 7: 13,746 crimes\nDistrict 8: 15,743 crimes\nDistrict 9: 11,031 crimes\nDistrict 10: 12,508 crimes\nDistrict 11: 18,735 crimes (highest)\nDistrict 12: 13,197 crimes\nDistrict 14: 9,296 crimes\nDistrict 15: 10,022 crimes\nDistrict 16: 8,326 crimes\nDistrict 17: 6,764 crimes\nDistrict 18: 15,181 crimes\nDistrict 19: 11,849 crimes\nDistrict 20: 4,389 crimes\nDistrict 22: 8,267 crimes\nDistrict 24: 8,032 crimes\nDistrict 25: 12,968 crimes\nDistrict 31: 7 crimes (lowest)\n\nDistrict 11 has the highest number of reported crimes (18,735), while District 31 has the lowest (7 crimes).",
        "score": 1
    },
    {
        "question": "Filter the dataset to only include rows where 'Arrest' is True. Print the IDs of the first five rows of the filtered dataset.",
        "ground_truth": "[12990873, 24886, 13195357, 11858992, 11839309]",
        "derivation": "",
        "type": "data curation",
        "subtype": "data filtering and partitioning",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "The IDs of the first five rows where arrests were made are:\n1. 12990873\n2. 24886\n3. 13195357\n4. 11858992\n5. 11839309",
        "score": 1
    },
    {
        "question": "Filter the dataset to exclude rows where 'Primary Type' is 'THEFT'. Print the number of remaining rows.",
        "ground_truth": "199030",
        "derivation": "",
        "type": "data curation",
        "subtype": "data filtering and partitioning",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "There are 199,030 rows in the dataset after excluding rows where 'Primary Type' is 'THEFT'.",
        "score": 1
    },
    {
        "question": "Ensure that all 'District' values are zero-padded to a 3-digit format (e.g., '007' instead of '7'). Print the first five updated values.",
        "ground_truth": "['006', '011', '019', '024', '006']",
        "derivation": "",
        "type": "data curation",
        "subtype": "data normalization and standardization",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "The first five District values have been successfully converted to a 3-digit zero-padded format:\n1. 006\n2. 011\n3. 019\n4. 024\n5. 006\n\nEach value is now represented with leading zeros to ensure a consistent 3-digit format, as requested.",
        "score": 1
    },
    {
        "question": "Print out the Date column of the first five rows of the dataset in JSON format with the Date formatted in the european format (dd/mm/yyyy hh:mm).",
        "ground_truth": "[\"21/04/2019\",\"17/08/2019\",\"16/03/2019\",\"20/03/2019\",\"21/07/2019\"]",
        "derivation": "",
        "type": "data curation",
        "subtype": "data normalization and standardization",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Here are the first five dates from the dataset in European format (dd/mm/yyyy hh:mm) in JSON format:\n{\n  \"0\": \"21/04/2019 12:30\",\n  \"1\": \"17/08/2019 13:14\",\n  \"2\": \"16/03/2019 11:35\",\n  \"3\": \"20/03/2019 01:00\",\n  \"4\": \"21/07/2019 14:00\"\n}\n\nThe dates have been successfully converted from the original American format (mm/dd/yyyy) to the European format (dd/mm/yyyy) and the times are shown in 24-hour format.",
        "score": 0
    },
    {
        "question": "Print all values that occur in the column about crime types.",
        "ground_truth": "['ROBBERY' 'OFFENSE INVOLVING CHILDREN' 'THEFT' 'SEX OFFENSE'\n 'CRIMINAL SEXUAL ASSAULT' 'BURGLARY' 'HOMICIDE' 'DECEPTIVE PRACTICE'\n 'BATTERY' 'OTHER OFFENSE' 'MOTOR VEHICLE THEFT' 'WEAPONS VIOLATION'\n 'STALKING' 'NARCOTICS' 'ASSAULT' 'CRIMINAL DAMAGE' 'CRIMINAL TRESPASS'\n 'CRIM SEXUAL ASSAULT' 'PUBLIC PEACE VIOLATION'\n 'INTERFERENCE WITH PUBLIC OFFICER' 'PROSTITUTION' 'LIQUOR LAW VIOLATION'\n 'ARSON' 'OBSCENITY' 'CONCEALED CARRY LICENSE VIOLATION' 'KIDNAPPING'\n 'GAMBLING' 'INTIMIDATION' 'HUMAN TRAFFICKING' 'NON-CRIMINAL'\n 'OTHER NARCOTIC VIOLATION' 'PUBLIC INDECENCY']",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Here are all the crime types that appear in the dataset:\n1. ROBBERY\n2. OFFENSE INVOLVING CHILDREN\n3. THEFT\n4. SEX OFFENSE\n5. CRIMINAL SEXUAL ASSAULT\n6. BURGLARY\n7. HOMICIDE\n8. DECEPTIVE PRACTICE\n9. BATTERY\n10. OTHER OFFENSE\n11. MOTOR VEHICLE THEFT\n12. WEAPONS VIOLATION\n13. STALKING\n14. NARCOTICS\n15. ASSAULT\n16. CRIMINAL DAMAGE\n17. CRIMINAL TRESPASS\n18. CRIM SEXUAL ASSAULT\n19. PUBLIC PEACE VIOLATION\n20. INTERFERENCE WITH PUBLIC OFFICER\n21. PROSTITUTION\n22. LIQUOR LAW VIOLATION\n23. ARSON\n24. OBSCENITY\n25. CONCEALED CARRY LICENSE VIOLATION\n26. KIDNAPPING\n27. GAMBLING\n28. INTIMIDATION\n29. HUMAN TRAFFICKING\n30. NON-CRIMINAL\n31. OTHER NARCOTIC VIOLATION\n32. PUBLIC INDECENCY\n\nNote: \"CRIMINAL SEXUAL ASSAULT\" and \"CRIM SEXUAL ASSAULT\" appear to be two variations of the same crime type in the data.",
        "score": 1
    },
    {
        "question": "Out of all unique values in the crime type column, is there a category that is very similar but has a typo or another feature where someone most likely made an error? yes or no?",
        "ground_truth": "yes",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "yes",
        "score": 1
    },
    {
        "question": "Print the datatype of each column as a dict.",
        "ground_truth": "{'ID': 'int64', 'Case Number': 'object', 'Date': 'object', 'Block': 'object', 'IUCR': 'object', 'Primary Type': 'object', 'Description': 'object', 'Location Description': 'object', 'Arrest': 'bool', 'Domestic': 'bool', 'Beat': 'int64', 'District': 'int64', 'Ward': 'float64', 'Community Area': 'int64', 'FBI Code': 'object', 'X Coordinate': 'float64', 'Y Coordinate': 'float64', 'Year': 'int64', 'Updated On': 'object', 'Latitude': 'float64', 'Longitude': 'float64', 'Location': 'object'}",
        "derivation": "",
        "type": "data curation",
        "subtype": "data quality assessment",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Here are the data types for each column in the DataFrame:\n- ID: int64 (integer)\n- Case Number: object (string)\n- Date: object (string)\n- Block: object (string)\n- IUCR: object (string)\n- Primary Type: object (string)\n- Description: object (string)\n- Location Description: object (string)\n- Arrest: bool (boolean)\n- Domestic: bool (boolean)\n- Beat: int64 (integer)\n- District: int64 (integer)\n- Ward: float64 (floating-point)\n- Community Area: int64 (integer)\n- FBI Code: object (string)\n- X Coordinate: float64 (floating-point)\n- Y Coordinate: float64 (floating-point)\n- Year: int64 (integer)\n- Updated On: object (string)\n- Latitude: float64 (floating-point)\n- Longitude: float64 (floating-point)\n- Location: object (string)",
        "score": 0
    },
    {
        "question": "Remove all rows, where over 10% of values are missing, and count the remaining rows.",
        "ground_truth": "259173",
        "derivation": "",
        "type": "data curation",
        "subtype": "data cleaning and deduplication",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "After removing all rows where over 10% of values are missing, the dataframe contains 259,173 rows.",
        "score": 1
    },
    {
        "question": "Identify and count records where the 'Case Number' format does not match the pattern 'XX999999'. Delete those values and count the number of rows",
        "ground_truth": "261524",
        "derivation": "",
        "type": "data curation",
        "subtype": "data cleaning and deduplication",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "{'type': 'error', 'error': {'details': None, 'type': 'overloaded_error', 'message': 'Overloaded'}}",
        "score": 0
    },
    {
        "question": "Create a new column IDXYEAR for each row by multiplying the ID with the YEAR column. Print out the first five values of IDXYEAR.",
        "ground_truth": "[23546419923, 26228572587, 23481971424, 23486027595, 23755404594]",
        "derivation": "",
        "type": "data curation",
        "subtype": "data transformation and aggregation",
        "difficulty": "easy",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "The first five values of the new IDXYEAR column are:\n1. 23546419923\n2. 26228572587\n3. 23481971424\n4. 23486027595\n5. 23755404594\n\nThese values were created by multiplying each ID with its corresponding Year value. For example, the first value (23546419923) comes from multiplying ID 11662417 by Year 2019.",
        "score": 1
    },
    {
        "question": "Group the data by crime type and calculate the percentage of each group rounded to two values after the comma. Then calculate the difference of the group with the largest and the smallest percentage. Print out this delta percentage rounded to one value after the comma.",
        "ground_truth": "23.9",
        "derivation": "",
        "type": "data curation",
        "subtype": "data transformation and aggregation",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "The difference between the highest and lowest crime type percentages is 23.9%.\n\nNote: The calculation was performed by:\n1. Converting the crime type counts to percentages (multiplied by 100)\n2. Rounding these percentages to 2 decimal places\n3. Finding the maximum and minimum percentages\n4. Subtracting the minimum from the maximum and rounding to 1 decimal place\nThe result shows that there is a 23.9 percentage point difference between the most common and least common types of crimes in the dataset.",
        "score": 1
    },
    {
        "question": "Create a new column IDXYEAR for each row by multiplying the ID with the YEAR column. Print out the average of IDXYEAR over all rows rounded to two values after comma.",
        "ground_truth": "23688888976.91",
        "derivation": "",
        "type": "data curation",
        "subtype": "data transformation and aggregation",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "23,688,888,976.91",
        "score": 0
    },
    {
        "question": "Create a new column IDXYEAR for each row by multiplying the ID with the YEAR column. Does it make sense to create such a new column, answer with YES or NO.",
        "ground_truth": "NO",
        "derivation": "REASONING",
        "type": "data curation",
        "subtype": "data logic explanation",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "NO\n\nThe creation of an IDXYEAR column by multiplying ID with YEAR doesn't make sense because:\n1. It combines two unrelated pieces of information (an identifier and a temporal value)\n2. The resulting values have no meaningful interpretation\n3. It doesn't add any analytical value to the dataset\n4. It unnecessarily consumes storage space",
        "score": 1
    },
    {
        "question": "Create a new column ArrestProbabilityForCrime by calculating the average Arrests for the commited crime type. Does such a value make sense?'",
        "ground_truth": "YES",
        "derivation": "REASONING",
        "type": "data curation",
        "subtype": "data logic explanation",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Yes, the arrest probability values make sense for several reasons:\n\n1. The probabilities range from 0 to 1 (0% to 100%), which is mathematically correct for probability values.\n\n2. The values align with what we might expect for different types of crimes:\n   - Crimes like NARCOTICS (99.97%), PROSTITUTION (100%), and GAMBLING (100%) have very high arrest rates, which makes sense as these are often caught \"in the act\"\n   - CONCEALED CARRY LICENSE VIOLATION (97.7%) and INTERFERENCE WITH PUBLIC OFFICER (96.1%) also have high arrest rates, which is logical as these involve direct interaction with law enforcement\n   - Property crimes like BURGLARY (5.7%) and THEFT (9.8%) have lower arrest rates, which is reasonable as these crimes often occur without witnesses present\n   - Violent crimes like HOMICIDE have moderate arrest rates (37.5%), reflecting the intensive investigation these cases receive but also the challenges in solving them\n\n3. The values provide useful insights for analysis:\n   - They can help predict the likelihood of an arrest for a given crime type\n   - They could be useful for law enforcement resource allocation\n   - They provide transparency about enforcement patterns\n\nThe column creation is therefore both technically correct and practically meaningful for analysis purposes.",
        "score": 1
    },
    {
        "question": "Create a new column CommunityCrimeRate by dividing 'Community Area' by 'District'. Does it make sense to create such a new column, answer with YES or NO.",
        "ground_truth": "NO",
        "derivation": "REASONING",
        "type": "data curation",
        "subtype": "data logic explaination",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "NO - Creating a CommunityCrimeRate by dividing Community Area by District does not make sense because both are identification numbers rather than measurements. A meaningful crime rate would instead need to be calculated using actual crime counts divided by population or area measurements.",
        "score": 1
    },
    {
        "question": "If the 'Year' column contains only one unique value (e.g., 2019), should it be retained as a feature in the dataset? Provide a 'Yes' or 'No' answer and justify your reasoning.",
        "ground_truth": "NO",
        "derivation": "A feature with only one unique value does not provide useful information to the model, so it should be removed.",
        "type": "data curation",
        "subtype": "data logic explaination",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "No, the Year column should not be retained as a feature in the dataset. The justification is:\n\n1. The Year column contains only a single unique value (2019), which means it has zero variance.\n2. Features with zero variance provide no discriminative power for any kind of analysis or modeling because they contain no variability to help distinguish between different observations.\n3. Including such a constant column would only increase the dimensionality of the dataset without providing any additional information or value.\n4. From a machine learning perspective, features that have the same value for all instances don't contribute to the model's learning process and can be safely removed to reduce noise and computational overhead.\n\nTherefore, the Year column can be dropped from the dataset without losing any meaningful information.",
        "score": 1
    },
    {
        "question": "Should missing values in the 'Latitude' and 'Longitude' columns be imputed using the mean of each column? Provide a 'Yes' or 'No' answer and justify your response.",
        "ground_truth": "No",
        "derivation": "Latitude and longitude represent geographic coordinates, and taking the mean may create an unrealistic or non-existent location.",
        "type": "data curation",
        "subtype": "data logic explaination",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "No, missing values in Latitude and Longitude columns should not be imputed using mean values. Here's why:\n\n1. Geographical Meaning: Latitude and Longitude coordinates represent specific physical locations. Using mean values would place missing incidents at an artificial \"average\" location that may not make any real-world sense (could be in the middle of a lake or building).\n\n2. Data Integrity: The missing coordinates appear to be missing together (both Latitude and Longitude are missing for the same 2,352 records), suggesting these are systematic omissions rather than random errors. This pattern indicates these values are likely intentionally not recorded rather than accidentally missing.\n\n3. Statistical Validity: Mean imputation would artificially cluster incidents at the mean location, distorting any spatial analysis and potentially leading to incorrect conclusions about crime patterns or hotspots.\n\n4. Better Alternatives: If location data is crucial, better approaches would be to:\n   - Use the Block information to approximate coordinates\n   - Remove records with missing coordinates if spatial analysis is needed\n   - Keep them as missing if the analysis doesn't require location data",
        "score": 1
    },
    {
        "question": "Classify the following crimes as 'Violent' or 'Non-Violent': 'Robbery', 'Burglary', 'Sexual Assault', 'Theft', 'Homicide', and 'Fraud'. Provide your answer as a dictionary with the crime as the key and 'Violent' or 'Non-Violent' as the value.",
        "ground_truth": "{ 'Robbery': 'Violent', 'Burglary': 'Non-Violent', 'Sexual Assault': 'Violent', 'Theft': 'Non-Violent', 'Homicide': 'Violent', 'Fraud': 'Non-Violent' }",
        "derivation": "Violent crimes typically involve direct harm or threat of harm to individuals, while non-violent crimes typically involve property or deception.",
        "type": "data curation",
        "subtype": "data logic explaination",
        "difficulty": "hard",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "{\n    'Robbery': 'Violent',\n    'Burglary': 'Non-Violent',\n    'Sexual Assault': 'Violent',\n    'Theft': 'Non-Violent',\n    'Homicide': 'Violent',\n    'Fraud': 'Non-Violent'\n}",
        "score": 1
    },
    {
        "question": "Which non-violent crime and which violent crime occur the most in the dataset? Print the results as 'Most Frequent Violent Crime: X' and 'Most Frequent Non-Violent Crime: Y'.",
        "ground_truth": "Most Frequent Violent Crime: BATTERY, Most Frequent Non-Violent Crime: THEFT",
        "derivation": "By categorizing crimes into violent and non-violent groups, we can count the frequency of each and identify the most common for both categories.",
        "type": "data curation",
        "subtype": "data logic explaination",
        "difficulty": "medium",
        "table_path": "./datasets/research/Crimes_-_2019.csv",
        "result": "Most Frequent Violent Crime: BATTERY\nMost Frequent Non-Violent Crime: THEFT",
        "score": 1
    }
]